import os
import re
import hashlib
import numpy as np
import pandas as pd
import faiss
import spacy
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from string import punctuation
from sentence_transformers import SentenceTransformer
from thefuzz import fuzz
from src.managers import DataManager

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
nlp = spacy.load("ru_core_news_sm")
dm = DataManager.initialize("price-list.xlsx")


class TextPreprocessor:
    def __init__(self, language='russian'):
        self.language = language
        self.stop_words = set(stopwords.words(language))
        self.punctuation = set(punctuation)

    def clean_text(self, text, is_article=False):
        if is_article:
            return text.strip().lower()
        return ''.join([ch if ch.isalnum() or ch.isspace() else ' ' for ch in text.lower()])

    def tokenize(self, text):
        return word_tokenize(text.lower())

    def remove_stopwords(self, tokens):
        return [word for word in tokens if word not in self.stop_words]

    def filter_punctuation(self, tokens):
        return [token for token in tokens if token not in self.punctuation]

    def preprocess(self, text, is_article=False):
        if not isinstance(text, str):
            text = str(text)
        if is_article:
            return text.strip().lower()
        cleaned = self.clean_text(text)
        tokens = self.tokenize(cleaned)
        tokens = self.remove_stopwords(tokens)
        tokens = self.filter_punctuation(tokens)
        return " ".join(tokens)


def filter_article(query: str) -> str:
    matches = re.findall(r'\b([A-Za-z–ê-–Ø–∞-—è0-9\-]{5,})\b', query)
    return max(matches, key=len) if matches else ""


def extract_entities(query: str):
    doc = nlp(query)
    return {ent.label_: ent.text for ent in doc.ents}


def classify_intent(query: str):
    query = query.lower()
    keywords = {
        "search_article": ["–∞—Ä—Ç–∏–∫—É–ª", "–∞—Ä—Ç.", "–∫–æ–¥ —Ç–æ–≤–∞—Ä–∞", "–Ω–æ–º–µ—Ä"],
        "search_name": ["–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "–Ω–∞–∑–≤–∞–Ω–∏–µ", "—á—Ç–æ –∑–∞ —Ç–æ–≤–∞—Ä"],
        "search_description": ["–æ–ø–∏—Å–∞–Ω–∏–µ", "—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏", "–ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏", "—Å–æ—Å—Ç–∞–≤"]
    }
    for intent, words in keywords.items():
        if any(word in query for word in words):
            return intent
    return "search_unknown"


def get_embedding_path(table_name, column_name):
    hash_name = hashlib.md5(f"{table_name}_{column_name}".encode()).hexdigest()
    return os.path.join("data", "embeddings", f"{hash_name}.npy")


def detect_target_column(query: str, table_name: str, dm: DataManager) -> str:
    article = filter_article(query)
    if article:
        return "–ê—Ä—Ç–∏–∫—É–ª"

    if any(word in query.lower() for word in ("–æ–ø–∏—Å–∞–Ω–∏–µ", "—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏")):
        return "–û–ø–∏—Å–∞–Ω–∏–µ"

    return "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ"


def search_in_single_column(query, table_name, column, model, preprocessor, top_k=5):
    print(f"[üîé] –ü–æ–∏—Å–∫ –≤ –∫–æ–ª–æ–Ω–∫–µ '{column}' –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")
    df = dm.get_table_data(table_name).copy()

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    df['clean_article'] = df['–ê—Ä—Ç–∏–∫—É–ª'].astype(str).str.strip().str.lower()

    # –¢–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –∞—Ä—Ç–∏–∫—É–ª–æ–≤
    if column == "–ê—Ä—Ç–∏–∫—É–ª":
        article = filter_article(query)
        if article:
            print(f"[üîç] –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞—Ä—Ç–∏–∫—É–ª–æ–≤: {df['–ê—Ä—Ç–∏–∫—É–ª'].head(3).tolist()}")

            # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            exact_match = df[df['clean_article'] == article.lower()]
            if not exact_match.empty:
                print(f"[‚úì] –ù–∞–π–¥–µ–Ω–æ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∞—Ä—Ç–∏–∫—É–ª–∞")
                exact_match['similarity'] = 1.0
                return exact_match.head(top_k)

            # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            partial_match = df[df['clean_article'].str.contains(article.lower())]
            if not partial_match.empty:
                print(f"[‚ö†] –ù–∞–π–¥–µ–Ω—ã —á–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∞—Ä—Ç–∏–∫—É–ª–æ–≤")
                partial_match['similarity'] = 0.9
                return partial_match.head(top_k)

    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
    cleaned = preprocessor.preprocess(query, is_article=(column == "–ê—Ä—Ç–∏–∫—É–ª"))
    q_emb = model.encode([cleaned])[0]
    q_emb /= np.linalg.norm(q_emb)

    try:
        emb_path = get_embedding_path(table_name, column)
        col_emb = np.load(emb_path)
        col_emb /= np.linalg.norm(col_emb, axis=1, keepdims=True)

        index = faiss.IndexFlatIP(col_emb.shape[1])
        index.add(col_emb)
        distances, indices = index.search(np.array([q_emb]), top_k)

        res = df.iloc[indices[0]].copy()
        res["similarity"] = distances[0]
        res["search_column"] = column
        return res

    except Exception as e:
        print(f"[‚ùå] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {str(e)}")
        return pd.DataFrame()


def search_smart(query, table_name, model, preprocessor, top_k=5):
    # –ü–æ–∏—Å–∫ –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É
    article = filter_article(query)
    if article:
        print(f"[üîé] –ü–æ–∏—Å–∫ –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É: {article}")
        df = dm.get_table_data(table_name).copy()
        df['clean_article'] = df['–ê—Ä—Ç–∏–∫—É–ª'].astype(str).str.strip().str.lower()

        # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        exact_match = df[df['clean_article'] == article.lower()].copy()
        if not exact_match.empty:
            print(f"[‚úì] –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∞—Ä—Ç–∏–∫—É–ª–∞: {article}")
            exact_match['similarity'] = 1.0
            exact_match['search_column'] = '–ê—Ä—Ç–∏–∫—É–ª'
            return exact_match.head(top_k)

        # –ù–µ—á–µ—Ç–∫–∏–π –ø–æ–∏—Å–∫
        print(f"[‚ö†] –¢–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –Ω–µ—Ç, –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∞—Ä—Ç–∏–∫—É–ª–æ–≤")
        fuzzy_df = df.copy()
        fuzzy_df['fuzzy_score'] = fuzzy_df['clean_article'].apply(
            lambda x: fuzz.ratio(x, article.lower())
        )
        fuzzy_results = fuzzy_df[fuzzy_df['fuzzy_score'] > 70].sort_values(
            'fuzzy_score', ascending=False
        ).head(top_k)

        if not fuzzy_results.empty:
            fuzzy_results = fuzzy_results.copy()
            fuzzy_results['similarity'] = fuzzy_results['fuzzy_score'] / 100
            return fuzzy_results

    # –ü–æ–∏—Å–∫ –ø–æ –Ω–∞–º–µ—Ä–µ–Ω–∏—é
    intent = classify_intent(query)
    print(f"[‚Ñπ] –û–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ: {intent}")

    if intent == "search_article":
        return search_in_single_column(query, table_name, "–ê—Ä—Ç–∏–∫—É–ª", model, preprocessor, top_k)
    elif intent == "search_description":
        return search_in_single_column(query, table_name, "–û–ø–∏—Å–∞–Ω–∏–µ", model, preprocessor, top_k)

    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫
    results = []
    for column in ["–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "–û–ø–∏—Å–∞–Ω–∏–µ", "–ê—Ä—Ç–∏–∫—É–ª"]:
        try:
            res = search_in_single_column(query, table_name, column, model, preprocessor, top_k)
            results.append(res)
        except Exception as e:
            print(f"[‚ö†] –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –∫–æ–ª–æ–Ω–∫–µ {column}: {str(e)}")

    final_results = pd.concat(results).sort_values(
        "similarity",
        ascending=False
    ).drop_duplicates(subset='–ê—Ä—Ç–∏–∫—É–ª').head(top_k)

    return final_results

    # –ü–æ–∏—Å–∫ –ø–æ –Ω–∞–º–µ—Ä–µ–Ω–∏—é
    intent = classify_intent(query)
    print(f"[‚Ñπ] –û–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ: {intent}")

    if intent == "search_article":
        return search_in_single_column(query, table_name, "–ê—Ä—Ç–∏–∫—É–ª", model, preprocessor, top_k)
    elif intent == "search_description":
        return search_in_single_column(query, table_name, "–û–ø–∏—Å–∞–Ω–∏–µ", model, preprocessor, top_k)

    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫
    results = []
    for column in ["–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "–û–ø–∏—Å–∞–Ω–∏–µ", "–ê—Ä—Ç–∏–∫—É–ª"]:
        try:
            res = search_in_single_column(query, table_name, column, model, preprocessor, top_k)
            results.append(res)
        except Exception as e:
            print(f"[‚ö†] –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –∫–æ–ª–æ–Ω–∫–µ {column}: {str(e)}")

    final_results = pd.concat(results).sort_values(
        "similarity",
        ascending=False
    ).drop_duplicates(subset='–ê—Ä—Ç–∏–∫—É–ª').head(top_k)

    return final_results


def main():
    preprocessor = TextPreprocessor(language="russian")
    model = SentenceTransformer("sberbank-ai/sbert_large_nlu_ru")

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    for table in dm.get_all_table_names():
        df = dm.get_table_data(table)
        for column in ["–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "–ê—Ä—Ç–∏–∫—É–ª", "–û–ø–∏—Å–∞–Ω–∏–µ"]:
            emb_path = get_embedding_path(table, column)
            if os.path.exists(emb_path):
                continue

            print(f"[‚è≥] –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {table}.{column}...")
            texts = df[column].astype(str)

            if column == "–ê—Ä—Ç–∏–∫—É–ª":
                pre_texts = [preprocessor.preprocess(t, is_article=True) for t in texts]
            else:
                pre_texts = [preprocessor.preprocess(t) for t in texts]

            embeddings = model.encode(pre_texts, show_progress_bar=True)
            os.makedirs(os.path.dirname(emb_path), exist_ok=True)
            np.save(emb_path, embeddings)
            print(f"[‚úì] –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {table}.{column}")

    # –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –ø–æ–∏—Å–∫–∞
    print("\n=== –í—ã–±–æ—Ä —Ç–∞–±–ª–∏—Ü—ã ===")
    tables = dm.get_all_table_names()
    for i, name in enumerate(tables, 1):
        print(f"{i}. {name}")

    table_idx = int(input("\n–í—ã–±–µ—Ä–∏—Ç–µ –Ω–æ–º–µ—Ä —Ç–∞–±–ª–∏—Ü—ã: ")) - 1
    selected_table = tables[table_idx]
    query = input("–í–≤–µ–¥–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å: ")

    results = search_smart(query, selected_table, model, preprocessor)

    print("\nüîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞:")
    if not results.empty:
        # –°–ø–∏—Å–æ–∫ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        required_columns = ['–ê—Ä—Ç–∏–∫—É–ª', '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–û–ø–∏—Å–∞–Ω–∏–µ', 'similarity', 'search_column']

        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏
        for col in required_columns:
            if col not in results.columns:
                results[col] = None

        # –í—ã–≤–æ–¥–∏–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        print(results[required_columns])
    else:
        print("–ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")


if __name__ == "__main__":
    main()